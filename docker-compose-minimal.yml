volumes:
  #ollama_storage:
  open-webui:
  flowise:

# x-ollama: &service-ollama
#   image: ollama/ollama:latest
#   container_name: ollama
#   restart: unless-stopped
#   expose:
#     - 11434/tcp
#   environment:
#     - OLLAMA_CONTEXT_LENGTH=8192
#     - OLLAMA_FLASH_ATTENTION=1
#     - OLLAMA_KV_CACHE_TYPE=q8_0
#     - OLLAMA_MAX_LOADED_MODELS=2
#   volumes:
#     - ollama_storage:/root/.ollama

# x-init-ollama: &init-ollama
#   image: ollama/ollama:latest
#   container_name: ollama-pull-llama
#   volumes:
#     - ollama_storage:/root/.ollama
#   entrypoint: /bin/sh
#   command:
#     - "-c"
#     - "sleep 3; OLLAMA_HOST=ollama:11434 ollama pull qwen2.5:7b-instruct-q4_K_M; OLLAMA_HOST=ollama:11434 ollama pull nomic-embed-text"

services:
  flowise:
    image: flowiseai/flowise
    restart: unless-stopped
    container_name: flowise
    ports:
      - 127.0.0.1:3001:3001
    environment:
      - PORT=3001
      - FLOWISE_USERNAME=${FLOWISE_USERNAME}
      - FLOWISE_PASSWORD=${FLOWISE_PASSWORD}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ~/.flowise:/root/.flowise
    entrypoint: /bin/sh -c "sleep 3; flowise start"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    container_name: open-webui
    ports:
      - 127.0.0.1:8080:8080
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui:/app/backend/data

  # ollama-cpu:
  #   profiles: [ "cpu" ]
  #   <<: *service-ollama
  #   ports:
  #     - 127.0.0.1:11434:11434

  # ollama-gpu:
  #   profiles: [ "gpu-nvidia" ]
  #   <<: *service-ollama
  #   ports:
  #     - 127.0.0.1:11434:11434
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [ gpu ]

  # ollama-gpu-amd:
  #   profiles: [ "gpu-amd" ]
  #   <<: *service-ollama
  #   image: ollama/ollama:rocm
  #   ports:
  #     - 127.0.0.1:11434:11434
  #   devices:
  #     - "/dev/kfd"
  #     - "/dev/dri"

  # ollama-pull-llama-cpu:
  #   profiles: [ "cpu" ]
  #   <<: *init-ollama
  #   depends_on:
  #     - ollama-cpu

  # ollama-pull-llama-gpu:
  #   profiles: [ "gpu-nvidia" ]
  #   <<: *init-ollama
  #   depends_on:
  #     - ollama-gpu

  # ollama-pull-llama-gpu-amd:
  #   profiles: [ gpu-amd ]
  #   <<: *init-ollama
  #   image: ollama/ollama:rocm
  #   depends_on:
  #     - ollama-gpu-amd